=== PAGE 1 ===

In this game, the agent plays as a chef who explores the city markets in order to find the best ingredients from a 
limited supply in order to cook delicious dishes. Different recipes require different combinations of ingredients, 
and some combinations are more valuable than others. However, certain ingredients are not fitting well together, 
causing failures or penalties.
The city map consists of a 5x5 grid where each cell represents a market selling an ingredient. The agent moves 
around the grid, collecting ingredients, and must decide when to stop collecting and attempt to create a dish. The 
goal is to maximize the total dish value while minimizing wasted ingredients.
Challenging variant
 
As an interesting variant, we could introduce a monetary budget of the chef, different costs for the ingredients, 
and a revenue for the dishes, depending on their quality. In this case, the goal could be to maximize the chef weal-
th after a certain number of rounds.
Tasks
 
Model the environment as an MDP, defining states, actions, and rewards.
Implement two RL agents: one using Q-learning or Sarsa with a tabular representation and another using 
function approximation (e.g., linear aproximation or neural networks).
Compare the learning performance and policy efficiency between the two approaches.
Analyze how state representation affects convergence speed and sample efficiency.
Project 2: The Gambler
 
Main Focus
 
Linear solutions and feature design.
Problem Description
 
Implement a reinforcement learning agent to play a simplified version of Blackjack. The agent must decide whe-
ther to "hit" or "stand" based on the game state.
Challenging variant
 
The agent is an expert gambler, so cheating is always an option, but there are risks! The idea is that a possible ac-
tion is always to peek at the next card in the deck. If this is successful you gain information that can be used to 
choose if play "hit" or "stand". But if the agent is caught there is a penalty.
Tasks
 
Model the environment, defining an appropriate state representation (e.g., player hand value, dealer's visible 
card, ace usability).
Implement an RL agent using Linear Function Approximation (LFA) to estimate state-action values.
